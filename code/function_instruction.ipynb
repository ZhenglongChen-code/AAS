{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "209b8f2725478b35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This file is mainly use to introduce how the fuctions and classes are used in our code.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22613f8495673813"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# statistics layer and function\n",
    "## Multi-dimensional gaussian distribution\n",
    "The aim of KRnet is mapping the pinn-loss disribution to a multi-dimensional standard normal distribution, in the code we construct a class to reperesent it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34bc6186b98b8ba8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:44:36.798585100Z",
     "start_time": "2024-07-14T07:44:34.044235700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DiagGaussian():\n",
    "    \"\"\"\n",
    "    Multivariate Gaussian distribution with diagonal covariance matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu, cov):\n",
    "        \"\"\"Constructor\n",
    "        Args:\n",
    "          shape: Tuple with shape of data, if int shape has one dimension\n",
    "          trainable: Flag whether to use trainable or fixed parameters\n",
    "        \"\"\"\n",
    "        # super().__init__()\n",
    "        self.shape = mu.shape\n",
    "        self.d = np.prod(self.shape)  # dim\n",
    "\n",
    "        self.loc = mu\n",
    "        self.scale = torch.sqrt(torch.diagonal(cov)).view(-1, )  # diagonal elements std: \\sigma_1, \\sigma_2  \n",
    "        self.log_scale = torch.log(self.scale)  # torch.zeros(1, *self.shape) # log sigma\n",
    "\n",
    "    def forward(self, num_samples=1):\n",
    "        eps = torch.randn(\n",
    "            (num_samples,) + self.shape, dtype=self.loc.dtype, device=self.loc.device\n",
    "        )  # generate samples satisfy N(0,1)\n",
    "        log_scale = self.log_scale  # log \\sigma_i\n",
    "\n",
    "        z = self.loc + self.scale * eps  # d-dimension variable obey multivariate Gaussian distribution\n",
    "        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n",
    "            log_scale + 0.5 * torch.pow(eps, 2), -1\n",
    "        )  # -0.5 denote log sqrt(2*pi)^{-1} = - 0.5 log 2*pi , log_p : log probability density function\n",
    "        return z, log_p\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        z, log_p = self.forward(sample_shape[0])\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        log_scale = self.log_scale\n",
    "        # qurad_form = torch.pow((z - self.loc) / torch.exp(log_scale), 2)\n",
    "        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n",
    "            log_scale + 0.5 * torch.pow((z - self.loc) / self.scale, 2),\n",
    "            dim=-1)\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "this is a standard multivariate gaussian distribution with 4 inner function.\n",
    "\n",
    "- `.__init__(mu, cov)` takes 2 vector mu, cov as initial parameters, generally mu is a 1-dim vector, (e.g. $[\\mu_1, \\mu_2]$); cov is covariance (e.g. $cov = \\begin{bmatrix} \\sigma_{11}, \\sigma_{12} \\\\ \\sigma_{21},\\sigma_{22} \\end{bmatrix}$), simply we only focus on Multivariate Gaussian distribution with diagonal covariance matrix. therefore, $\\sigma_{ij}=0, i \\neq j$, e.g. cov = [[1,0],[0,1]]\n",
    "- `.forward(num_samples=1)` takes 1 parameter: `num_samples` to generate `num_samples` data which satisfies the initialized multivariate Gaussian distribution. It firstly generate n samples satisfy multivariate standard gaussian distribution. then $Z = \\sigma X + \\mu, X \\sim (0, 1), Z \\sim (\\mu, \\sigma^2)$, then return Z and log probability density.\n",
    "- `.sample(self, sample_shape=torch.Size())` take a tuple or list parameter(e.g. (n_sample,) ), using `.forward()`function to generate samples.\n",
    "- `.log_prob(self, z)` using sample data `z` to compute log Joint probability density. Since Z satisfies iid components multivariate gaussian distribution, but such distribution can generate infinity samples, so the likelihood probability can be replace by probability density.\n",
    "\n",
    "$$\n",
    "f(x_1,\\cdots, x_k) = \\prod_{i=1}^{k} f(x_i) =\\prod_{i=1}^k \\dfrac{1}{\\sqrt{2\\pi}\\sigma_i} e^{-\\frac{(x_i -\\mu)^2}{2\\sigma_i^2}} = (2\\pi)^{-\\frac{k}{2}} \\prod_{i=1}^k \\sigma_i^{-1} e^{-\\frac{(x_i -\\mu)^2}{2\\sigma_i^2}} \\\\\n",
    "\\log f(x_1,\\cdots, x_k) = -\\frac{k}{2} * \\log (2\\pi) - \\sum_i (\\log \\sigma_i +\\frac{(x_i -\\mu)^2}{2\\sigma_i^2} )\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a231bf015dbb6a30"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([[ 1.1284,  5.3211],\n",
      "        [ 0.5985,  2.6812],\n",
      "        [-0.0897,  0.7149],\n",
      "        [ 1.7716,  2.7921],\n",
      "        [ 2.4565,  0.6586],\n",
      "        [ 1.2330, -0.0736],\n",
      "        [ 1.9474,  1.2989],\n",
      "        [-0.2418,  3.0620],\n",
      "        [ 0.8282,  3.1133],\n",
      "        [ 1.5985,  3.5932]])\n",
      "log_p: tensor([-4.2337, -2.5451, -3.2562, -2.7894, -3.7478, -3.1310, -2.9179, -3.3462,\n",
      "        -2.6085, -2.9893])\n",
      "log_prob:tensor([-4.2337, -2.5451, -3.2562, -2.7894, -3.7478, -3.1310, -2.9179, -3.3462,\n",
      "        -2.6085, -2.9893])\n"
     ]
    }
   ],
   "source": [
    "#  举例生成一个2元高斯类，并分别调用上述方法\n",
    "mu = torch.tensor([1.0, 2.0])\n",
    "cov = torch.tensor([[1.0, 0], [0, 3.0]])  # diagonal Gaussian\n",
    "gaussian_dist = DiagGaussian(mu, cov)\n",
    "z, log_p = gaussian_dist.forward(num_samples=10)\n",
    "print(f\"z: {z}\")\n",
    "print(f\"log_p: {log_p}\")\n",
    "print('log_prob:{}'.format(gaussian_dist.log_prob(z)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T08:10:05.592971500Z",
     "start_time": "2024-07-14T08:10:05.586053600Z"
    }
   },
   "id": "441dab5e742697fb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1.1231, 2.3162]), tensor([0.8700, 1.6421]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(z, dim=0), torch.std(z, dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T08:10:07.857170600Z",
     "start_time": "2024-07-14T08:10:07.846448400Z"
    }
   },
   "id": "9fbdd4028700ff51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KRnet structure\n",
    "## AffineCoupling layer\n",
    "This layer is an important layer in real NVP, this layer is used to map X to Z, a single affinecoupling layer can be seen as $f_{[i]}$ as below.\n",
    "\n",
    "$$\n",
    "z=f(x)=f_{[L]}\\circ\\ldots\\circ f_{[1]}(x)\\quad\\mathrm{and}\\quad x=f^{-1}(z)=f_{[1]}^{-1}\\circ\\ldots\\circ f_{[L]}^{-1}(z),\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87dd908096a95b4e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    \"\"\" Affine Coupling Layers \n",
    "    Args:\n",
    "        input_size: input var dimension, size of input tensor: such as X in R^D , then input_size = D,\n",
    "        split_size: split size of input var : X = [X[0:d] , X[d:D]], split_size= len(x1) = d\n",
    "        hidden_size: width of hidden layers\n",
    "        n_hidden: depth of hidden layers\n",
    "        cond_label_size: condition variable size\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, split_size, hidden_size, n_hidden, cond_label_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.log_beta = nn.Parameter(torch.zeros(input_size-split_size, dtype=torch.float32))\n",
    "        self.input_size = input_size\n",
    "        self.split_size = split_size\n",
    "        net = [nn.Linear(split_size + (cond_label_size if cond_label_size is not None else 0), hidden_size+split_size + (cond_label_size if cond_label_size is not None else 0)), \n",
    "                nn.ReLU(), \n",
    "                nn.Linear(hidden_size + split_size + (cond_label_size if cond_label_size is not None else 0), hidden_size)]\n",
    "        for _ in range(n_hidden):\n",
    "            net += [nn.ReLU(),nn.Linear(hidden_size, hidden_size)]# ResNet_block(hidden_size, hidden_size)\n",
    "        net += [nn.ReLU(), nn.Linear(hidden_size, 2*(input_size-split_size))]\n",
    "        # 2*(input_size-split_size), half for s_i, half for t_i\n",
    "        self.net = nn.Sequential(*net)\n",
    "        self.alpha = 0.6\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\"\n",
    "        x: torch tensor, (N, d)\n",
    "        y: torch tensor, (N, cond_dim), condition var\n",
    "        \"\"\"\n",
    "        x1 = (x[:,:self.split_size]).view(-1,self.split_size)\n",
    "        x2 = (x[:,self.split_size:]).view(-1,self.input_size-self.split_size)\n",
    "  \n",
    "        h = self.net(x1 if y is None else torch.cat([x1,y], dim=1))\n",
    "        s = h[:, :self.input_size-self.split_size]\n",
    "        s = s.view(-1, self.input_size-self.split_size)\n",
    "        t = h[:,self.input_size-self.split_size:] \n",
    "        t = t.view(-1, self.input_size-self.split_size)\n",
    "\n",
    "        u2 = x2 + (self.alpha*x2*torch.tanh(s) + torch.exp(torch.clip(self.log_beta, -5.0, 5.0)) * torch.tanh(t))\n",
    "\n",
    "        log_abs_det_jacobian = torch.log(1+self.alpha*torch.tanh(s))  # 比论文里多写一个log\n",
    "        log_abs_det_jacobian = log_abs_det_jacobian.sum(dim=1)\n",
    "        return torch.cat([x1, u2], dim=-1), log_abs_det_jacobian\n",
    "\n",
    "    def inverse(self, u, y=None):\n",
    "        u1 = (u[:,:self.split_size]).view(-1,self.split_size)\n",
    "        u2 = (u[:,self.split_size:]).view(-1,self.input_size-self.split_size)\n",
    "\n",
    "        h = self.net(u1 if y is None else torch.cat([u1,y], dim=1))\n",
    "        s = h[:,:self.input_size-self.split_size]\n",
    "        s = s.view(-1, self.input_size-self.split_size)\n",
    "        t = h[:,self.input_size-self.split_size:] \n",
    "        t = t.view(-1, self.input_size-self.split_size)\n",
    "\n",
    "        x2 = (u2 - torch.exp(torch.clip(self.log_beta, -5.0, 5.0))*torch.tanh(t))/(1 + self.alpha*torch.tanh(s))\n",
    "        log_abs_det_jacobian = -torch.log(1 + self.alpha*torch.tanh(s))  # 原本是倒数， 取对数后变为负数\n",
    "        log_abs_det_jacobian = log_abs_det_jacobian.sum(dim=1)\n",
    "        return torch.cat([u1,x2],dim=-1), log_abs_det_jacobian"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T09:01:36.421613500Z",
     "start_time": "2024-07-14T09:01:36.419597700Z"
    }
   },
   "id": "6dbeda13d9e5294b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "this layer have a special structure, all of its layer are reversible and have explicit derivation.\n",
    "\n",
    "$$\n",
    "p_X(x)=p_Z(f(x))|\\det\\nabla_xf|. \\\\\n",
    "|\\det\\nabla_xf|=\\prod_{i=1}^L\\left|\\det\\nabla_{x_{[i-1]}}f_{[i]}\\right|,\n",
    "$$\n",
    "\n",
    "we indicate $x_{[i-1]}$ as the intermediate variables with $x_{[0]} = input:x; \\quad  x_{[L]}=output:z.$ For each layer input $x_{[i]} \\in R^m$ and it will divide into 2 parts $x_{[i,1]} \\in R^d, x_{[i,2]} \\in R^{m-d}.$\n",
    "\n",
    "we define the affinecoupling layer with 3 function as follow:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}&x_{[i],1}=x_{[i-1],1}\\\\&x_{[i],2}=x_{[i-1],2}\\odot\\left(1+\\alpha\\tanh(s_{i}(x_{[i-1],1}))\\right)+e^{\\beta_{i}}\\odot\\tanh(t_{i}(x_{[i-1],1})), \\\\&(s_i,t_i)=\\text{NN}_{[i]}(x_{[i-1],1}).\\end{aligned}\n",
    "$$\n",
    "\n",
    "an input vector x go through this affinecoupling layer and return [y1, y2], in above $s_i, t_i$ have the same shape as $x_2$, split_size = len(x1). Since x2 will take Hadamard product with $s_i$ and $t_i$, output size of NN is 2 * len(x2) = 2*(m-d)\n",
    "\n",
    "cond_label_size=None, and it is useless in DAS experiment.\n",
    "\n",
    "- `__init__(input_size, split_size, hidden_size, n_hidden, cond_label_size=None)` input_size = len(x) = m, split_size = len(x1) = d, hidden_size: related to neural network hidden size, n_hidden: depth of hidden layers. it has a trainable parameter $\\log \\beta$ and a neural network with input_size=d, output_size=2*(m-d), len(s)=len(t)=m-d. network flow: x --> NN --> s\n",
    "- `forward(self, x, y=None)` : it takes x as input and use affinecoupling layer to compute s, t, then use above equation to compute y2. Lastly, return [y1, y2] and log_abs_det_jacobian: $\\log\\left|\\det\\nabla_{x}f(x)\\right|$. The detail compute progress:\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}, f(x) = f\\begin{bmatrix} & x_1 \\\\ & x_2\\end{bmatrix} = \\begin{bmatrix}x_1 \\\\ x_{2}=x_{2}\\odot\\left(1+\\alpha\\tanh(s(x_{1}))\\right)+e^{\\beta}\\odot\\tanh(t_(x_{1}))\\end{bmatrix} \\\\\n",
    "\n",
    "\\nabla_x f(x) = \\begin{bmatrix}1 & 0 \\\\ \\nabla_{x_1}f(x_2) & 1 + \\alpha \\tanh(s(x_1)) \\end{bmatrix}, \\quad \\det \\nabla_x f(x) = 1 + \\alpha \\tanh(s(x_1))\n",
    "$$\n",
    "\n",
    "- `inverse(self, u, y=None)` use u = [y1, y2] to compute original [x1, x2] and log_abs_det_jacobian: $\\log\\left|\\det\\nabla_{y}f^{-1}(y)\\right|$.\n",
    "  $$\n",
    "  \\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix} = f^{-1} \\begin{bmatrix}y_1 \\\\ y_2\\end{bmatrix} = \\begin{bmatrix}y_1 \\\\ \\frac{y_2 -e^{\\beta}\\odot\\tanh(t_(y_{1}))}{1 + \\alpha \\tanh(s(y_1))} \\end{bmatrix} \\\\\n",
    "\n",
    "  \\det \\nabla_y f^{-1}(y) = \\det \\begin{bmatrix}1 & 0 \\\\ \\nabla_{y_1}f(y_2) & \\frac{1}{1 + \\alpha \\tanh(s(y_1))} \\end{bmatrix} = \\frac{1}{1 + \\alpha \\tanh(s(y_1))}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75b994d351ddd123"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:tensor([[ 0.6809, -0.6300,  0.6180,  0.3619,  0.6358, -0.7499,  0.3838, -1.3763,\n",
      "         -0.2342,  0.1826, -0.6112, -0.0753, -1.1269],\n",
      "        [ 1.2290,  1.0808, -0.8349,  0.1110,  0.2813, -0.4483, -0.3334, -1.5100,\n",
      "          0.8289, -0.0202,  1.6570, -1.6118,  0.6263],\n",
      "        [ 0.2997, -0.5315,  0.0140, -0.2803, -0.3386,  0.0753,  0.0115,  0.6755,\n",
      "         -0.3715,  0.1363, -0.4698,  3.4673, -0.7533],\n",
      "        [ 0.5211,  1.4148, -1.1961,  0.6467,  0.1610,  0.4431, -0.4054, -1.1819,\n",
      "          0.3483, -0.9875, -1.1202,  1.7186, -1.4876]], grad_fn=<CatBackward0>), output_shape:torch.Size([4, 13])\n",
      "Log-Abs-Det-Jacobian: tensor([0.0968, 0.0934, 0.0987, 0.0807], grad_fn=<SumBackward1>)\n",
      "Inverse Output: tensor([[ 0.6809, -0.6300,  0.6180,  0.3619,  0.6358, -0.6864,  0.5352, -1.3314,\n",
      "         -0.3420,  0.1165, -0.6677, -0.1778, -0.9565],\n",
      "        [ 1.2290,  1.0808, -0.8349,  0.1110,  0.2813, -0.4255, -0.2127, -1.4724,\n",
      "          0.6193, -0.0952,  1.9261, -1.7578,  0.5844],\n",
      "        [ 0.2997, -0.5315,  0.0140, -0.2803, -0.3386,  0.0304,  0.1498,  0.8164,\n",
      "         -0.4634,  0.0686, -0.5077,  3.4513, -0.6277],\n",
      "        [ 0.5211,  1.4148, -1.1961,  0.6467,  0.1610,  0.3520, -0.2868, -1.1346,\n",
      "          0.1840, -1.1047, -1.2372,  1.6765, -1.2798]], grad_fn=<CatBackward0>)\n",
      "Inverse Log-Abs-Det-Jacobian: tensor([-0.0968, -0.0934, -0.0987, -0.0807], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 AffineCoupling 层实例\n",
    "input_size = 13\n",
    "split_size = 5\n",
    "hidden_size = 20\n",
    "n_hidden = 2\n",
    "cond_label_size = None  # 条件变量的大小\n",
    "\n",
    "affine_coupling = AffineCoupling(input_size, split_size, hidden_size, n_hidden, cond_label_size)\n",
    "\n",
    "# 示例输入数据\n",
    "x = torch.randn(4, input_size)  # 输入张量，形状为 (N, input_size)\n",
    "# y = torch.randn(4, cond_label_size)  # 条件变量，形状为 (N, cond_label_size)\n",
    "\n",
    "# 前向传播\n",
    "output, log_det_jacobian = affine_coupling(x)\n",
    "print(\"Output:{}, output_shape:{}\".format(output, output.shape))\n",
    "print(\"Log-Abs-Det-Jacobian:\", log_det_jacobian)\n",
    "\n",
    "# 逆向传播\n",
    "inverse_output, inverse_log_det_jacobian = affine_coupling.inverse(output)\n",
    "print(\"Inverse Output:\", inverse_output)\n",
    "print(\"Inverse Log-Abs-Det-Jacobian:\", inverse_log_det_jacobian)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T12:29:10.690585200Z",
     "start_time": "2024-07-14T12:29:10.682065300Z"
    }
   },
   "id": "7edf4d9b0681b2b3"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Linear(in_features=5, out_features=25, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=25, out_features=20, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=20, out_features=20, bias=True)\n  (5): ReLU()\n  (6): Linear(in_features=20, out_features=20, bias=True)\n  (7): ReLU()\n  (8): Linear(in_features=20, out_features=16, bias=True)\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_coupling.net"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T12:29:13.434132500Z",
     "start_time": "2024-07-14T12:29:13.427351800Z"
    }
   },
   "id": "b9150d37e7242db1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## squeezing layer\n",
    "Squeezing layer $L_S$ is used to deactivate some dimensions using a mask\n",
    "\n",
    "$$\n",
    "q=[\\underbrace{1,\\ldots,1,\\underbrace{0,\\ldots,0}_{d-n}]^{\\mathsf{T}},}_{n}\n",
    "$$\n",
    "\n",
    "the components $q \\odot x$ will keep being updated at the layer or net after, and the rest components $(1 − q)  \\odot x$ will be fixed from then on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f01096eae20847cb"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class squeezing(nn.Module):\n",
    "    \"\"\" KRnet squeezing layer\n",
    "    Args:\n",
    "        input_size: features of input x, dtype=int,  value should be the same as the x.shape(-1)\n",
    "        n_cut: number of elements to keep same in x after squeezing\n",
    "    \n",
    "    forward func: input x return q * x\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, n_cut=1):\n",
    "        super().__init__()\n",
    "        self.data_init = True\n",
    "        self.input_size = input_size\n",
    "        self.n_cut = n_cut\n",
    "        self.x = None  \n",
    "\n",
    "    def forward(self, x):    \n",
    "        # log_det = torch.zeros()\n",
    "        n_dim = x.shape[-1]  # n_dim = len(x)\n",
    "        if n_dim<self.n_cut:\n",
    "            raise Exception(\"Input dimension is less than n_cut.\")\n",
    "        if self.input_size==n_dim:  \n",
    "            if self.input_size>self.n_cut:  # len(x) > n_cut\n",
    "                if self.x is not None:\n",
    "                    raise Exception(\"x is already set.\")\n",
    "                else:\n",
    "                    self.x = x[:,(n_dim-self.n_cut):]  # 输入x的后n_cut个元素\n",
    "                    z = x[:,:(n_dim-self.n_cut)]  # 前d-n_cut个元素\n",
    "            else:\n",
    "                self.x = None\n",
    "        elif n_dim<=self.n_cut:  # len(x) < n_cut,\n",
    "            z = torch.cat([x, self.x], dim=-1)  # return original x, since self.x=None\n",
    "            self.x = None\n",
    "        else:\n",
    "            cut = x[:, (n_dim-self.n_cut):]\n",
    "            self.x = torch.cat([cut,self.x], dim=-1)\n",
    "            z = x[:,:(n_dim-self.n_cut)]\n",
    "        return z, 0\n",
    "    def inverse(self, z):\n",
    "        n_dim = z.shape[-1]\n",
    "        if self.input_size == n_dim:\n",
    "            n_start = self.input_size % self.n_cut\n",
    "            if n_start == 0:\n",
    "                n_start+=self.n_cut\n",
    "            self.x = z[:, n_start:]\n",
    "            x = z[:,:n_start]  # 输入的前n个元素\n",
    "        else:\n",
    "            x_length = self.x.shape[-1]\n",
    "            if x_length<self.n_cut:\n",
    "                raise Exception()\n",
    "  \n",
    "            cut = self.x[:, :self.n_cut]\n",
    "            x = torch.cat([z, cut],dim=-1)\n",
    "            if (x_length-self.n_cut)==0:\n",
    "                self.x = None\n",
    "            else:\n",
    "                self.x = self.x[:, self.n_cut:]\n",
    "        return x, 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T13:50:17.948693500Z",
     "start_time": "2024-07-14T13:50:17.946177100Z"
    }
   },
   "id": "4ffa0a06d2921cf7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Dimension Output: torch.Size([4, 7])\n",
      "Recovered Dimension Output: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义输入数据\n",
    "input_data = torch.randn(4, 10)  # (N, d) = (4, 10)\n",
    "\n",
    "# 创建 squeezing 层实例\n",
    "input_size = 10\n",
    "n_cut = 3\n",
    "squeezing_layer = squeezing(input_size, n_cut)\n",
    "\n",
    "# 正向传播：降维操作\n",
    "z, log_det_jacobian = squeezing_layer(input_data)\n",
    "print(\"Reduced Dimension Output:\", z.shape)  # 应输出 (4, 7)\n",
    "\n",
    "# 逆向传播：恢复维度操作\n",
    "recovered_data, inv_log_det_jacobian = squeezing_layer.inverse(z)\n",
    "print(\"Recovered Dimension Output:\", recovered_data.shape)  # 应输出 (4, 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T13:51:57.767525800Z",
     "start_time": "2024-07-14T13:51:57.759605700Z"
    }
   },
   "id": "22300117288830f1"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data: tensor([[ 0.7082,  1.1445, -0.3994, -0.6982, -0.0995, -0.1144,  1.5174,  1.9414,\n",
      "          2.0836, -0.4468],\n",
      "        [-0.3672,  0.8286, -0.9860,  0.4427, -0.9063, -0.2995,  0.1316,  0.8153,\n",
      "          0.6860,  0.0079],\n",
      "        [-0.9210, -0.0178, -1.1062, -1.5423, -2.1283,  0.0943,  1.4507,  1.0508,\n",
      "          0.0204,  0.6800],\n",
      "        [-0.0217, -0.1021, -0.8037,  1.2952, -0.8996,  0.1081,  0.8332,  0.2674,\n",
      "          0.1568, -0.9692]])\n",
      "Recovered data: tensor([[ 0.7082,  1.1445, -0.3994, -0.6982, -0.0995, -0.1144,  1.5174,  1.9414,\n",
      "          2.0836, -0.4468],\n",
      "        [-0.3672,  0.8286, -0.9860,  0.4427, -0.9063, -0.2995,  0.1316,  0.8153,\n",
      "          0.6860,  0.0079],\n",
      "        [-0.9210, -0.0178, -1.1062, -1.5423, -2.1283,  0.0943,  1.4507,  1.0508,\n",
      "          0.0204,  0.6800],\n",
      "        [-0.0217, -0.1021, -0.8037,  1.2952, -0.8996,  0.1081,  0.8332,  0.2674,\n",
      "          0.1568, -0.9692]])\n"
     ]
    }
   ],
   "source": [
    "print('input_data:', input_data)\n",
    "print('Recovered data:', recovered_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T13:53:42.070294500Z",
     "start_time": "2024-07-14T13:53:42.046728700Z"
    }
   },
   "id": "2ef07d55135eb40a"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.7082,  1.1445, -0.3994, -0.6982, -0.0995, -0.1144,  1.5174],\n        [-0.3672,  0.8286, -0.9860,  0.4427, -0.9063, -0.2995,  0.1316],\n        [-0.9210, -0.0178, -1.1062, -1.5423, -2.1283,  0.0943,  1.4507],\n        [-0.0217, -0.1021, -0.8037,  1.2952, -0.8996,  0.1081,  0.8332]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:13:52.777158400Z",
     "start_time": "2024-07-14T14:13:52.770150700Z"
    }
   },
   "id": "ddb1596fcb14437b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "    \"\"\" ActNorm layer, scale-bias layer \n",
    "    Args:\n",
    "        input_size: input size of input var\n",
    "        scale: scale parameter, default 1.0\n",
    "        logscale_factor: log scale parameter, default 3.0\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, scale=1.0, logscale_factor=3.0):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.logscale_factor = logscale_factor\n",
    "        self.data_init = True\n",
    "\n",
    "        self.b = nn.Parameter(torch.zeros(1, input_size))\n",
    "        self.register_buffer('b_init', torch.zeros(1, input_size))  # b_init 初始化为0\n",
    "        self.logs = nn.Parameter(torch.zeros(1, input_size))\n",
    "        self.register_buffer('logs_init', torch.zeros(1, input_size)) # logs_init 初始化为0\n",
    "\n",
    "    def forward(self, x, cond_y=None):\n",
    "        if not self.data_init:\n",
    "            x_mean = torch.mean(x, 0, keepdim=True)\n",
    "            x_var = torch.mean(torch.square(x - x_mean), [0], keepdim=True)\n",
    "\n",
    "            self.b_init = -x_mean\n",
    "            self.logs_init = torch.log(self.scale / (torch.sqrt(x_var) + 1e-6)) / self.logscale_factor\n",
    "\n",
    "            self.data_init = True\n",
    "        y = x + self.b + self.b_init\n",
    "        y = y * torch.exp(torch.clip(self.logs + self.logs_init, -5., 5.))\n",
    "\n",
    "        log_abs_det_jacobian = torch.clip(self.logs + self.logs_init, -5., 5.)\n",
    "        return y, log_abs_det_jacobian.expand_as(x).sum(dim=-1)\n",
    "\n",
    "    def inverse(self, y, cond_y=None):\n",
    "        x = y * torch.exp(-torch.clip(self.logs + self.logs_init, -5., 5.))\n",
    "        x = x - (self.b + self.b_init)\n",
    "        log_abs_det_jacobian = -torch.clip(self.logs + self.logs_init, -5., 5.)\n",
    "        return x, log_abs_det_jacobian.expand_as(x).sum(dim=-1)\n",
    "        \n",
    "    def reset_data_initialization(self):\n",
    "        self.data_init = False\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T10:59:06.640668600Z",
     "start_time": "2024-07-15T10:59:05.409120100Z"
    }
   },
   "id": "518b92ecfa9ce9e6"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Output: tensor([[ 0.0238,  0.4029,  0.3562,  0.6513,  1.2509, -1.2294, -0.2700,  0.9080,\n",
      "          0.1558,  1.9286],\n",
      "        [-0.9958, -1.0999, -2.1106, -0.6710, -1.0480,  0.5472, -0.7885,  0.2368,\n",
      "          2.0662,  0.6412],\n",
      "        [ 0.4584,  0.1916,  0.0611,  0.1501, -0.9073,  2.4289,  0.7725, -0.9598,\n",
      "         -0.2130, -1.1269],\n",
      "        [ 1.5795, -0.6697,  0.3275, -1.0277, -0.1423, -0.0347, -0.0460, -0.3218,\n",
      "         -0.7286, -0.2763]], grad_fn=<MulBackward0>)\n",
      "Log-Abs-Det-Jacobian: tensor([0., 0., 0., 0.], grad_fn=<SumBackward1>)\n",
      "Recovered Output: tensor([[ 0.0238,  0.4029,  0.3562,  0.6513,  1.2509, -1.2294, -0.2700,  0.9080,\n",
      "          0.1558,  1.9286],\n",
      "        [-0.9958, -1.0999, -2.1106, -0.6710, -1.0480,  0.5472, -0.7885,  0.2368,\n",
      "          2.0662,  0.6412],\n",
      "        [ 0.4584,  0.1916,  0.0611,  0.1501, -0.9073,  2.4289,  0.7725, -0.9598,\n",
      "         -0.2130, -1.1269],\n",
      "        [ 1.5795, -0.6697,  0.3275, -1.0277, -0.1423, -0.0347, -0.0460, -0.3218,\n",
      "         -0.7286, -0.2763]], grad_fn=<SubBackward0>)\n",
      "Inverse Log-Abs-Det-Jacobian: tensor([0., 0., 0., 0.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# 定义 ActNorm 层\n",
    "input_size = 10\n",
    "actnorm_layer = ActNorm(input_size)\n",
    "\n",
    "# 示例输入数据\n",
    "x = torch.randn(4, input_size)\n",
    "\n",
    "# 正向传播\n",
    "y, log_det = actnorm_layer(x)\n",
    "print(\"Transformed Output:\", y)\n",
    "print(\"Log-Abs-Det-Jacobian:\", log_det)\n",
    "\n",
    "# 逆向传播\n",
    "x_recovered, inv_log_det = actnorm_layer.inverse(y)\n",
    "print(\"Recovered Output:\", x_recovered)\n",
    "print(\"Inverse Log-Abs-Det-Jacobian:\", inv_log_det)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T10:59:13.160149Z",
     "start_time": "2024-07-15T10:59:13.152785800Z"
    }
   },
   "id": "bc65c7b13498a4ef"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actnorm_layer.b_init"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T11:00:55.204675700Z",
     "start_time": "2024-07-15T11:00:55.193133600Z"
    }
   },
   "id": "a097980aca597a3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non-linear CDF layers "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2725a4ed0f09a689"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class CDF_quadratic(nn.Module):\n",
    "    \"\"\"\n",
    "    Non-linear CDF layers\n",
    "    Here, the domain means the range of input variables\n",
    "    n_bins: number of bins for discreting the domain or range\n",
    "    input_dim: input var's dimension\n",
    "    r: for generating the mesh for discreting domain\n",
    "    bound: bound of domain\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bins, input_dim, r=1.2, bound=50.0, **kwargs):\n",
    "        super(CDF_quadratic, self).__init__(**kwargs)\n",
    "\n",
    "        assert n_bins % 2 == 0\n",
    "\n",
    "        self.n_bins = n_bins\n",
    "        self.input_dim = input_dim\n",
    "        # generate a nonuniform mesh symmetric to zero,\n",
    "        # and increasing by ratio r away from zero.\n",
    "        self.bound = bound\n",
    "        self.r = r\n",
    "\n",
    "        m = n_bins/2\n",
    "        x1L = bound*(r-1.0)/(np.power(r, m)-1.0)\n",
    "\n",
    "        index = torch.reshape(torch.arange(0, self.n_bins+1, dtype=torch.float32),(-1,1))\n",
    "        index -= m\n",
    "        xr = torch.where(index>=0, (1.-torch.pow(r, index))/(1.-r),\n",
    "                      (1.-torch.pow(r,torch.abs(index)))/(1.-r))\n",
    "        xr = torch.where(index>=0, x1L*xr, -x1L*xr)\n",
    "        xr = torch.reshape(xr,(-1,1))\n",
    "        xr = (xr + bound)/2.0/bound\n",
    "\n",
    "        self.x1L = x1L/2.0/bound\n",
    "        mesh = torch.cat([torch.reshape(torch.tensor([0.0]),(-1,1)), torch.reshape(xr[1:-1,0],(-1,1)), torch.reshape(torch.tensor([1.0]),(-1,1))],0) \n",
    "        self.register_buffer('mesh', mesh)\n",
    "        elmt_size = torch.reshape(self.mesh[1:] - self.mesh[:-1],(-1,1))\n",
    "        self.register_buffer('elmt_size', elmt_size)\n",
    "        self.p = nn.Parameter(torch.zeros(self.n_bins-1, input_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        self._pdf_normalize()\n",
    "        # rescale such points in [-bound, bound] will be mapped to [0,1]\n",
    "        x = (x + self.bound) / 2.0 / self.bound\n",
    "\n",
    "        # cdf mapping\n",
    "        x, logdet = self._cdf(x)\n",
    "  \n",
    "        # maps [0,1] back to [-bound, bound]\n",
    "        x = x * 2.0 * self.bound - self.bound\n",
    "        return x, logdet\n",
    "    def inverse(self, z, t=None):\n",
    "        self._pdf_normalize()\n",
    "        # rescale such points in [-bound, bound] will be mapped to [0,1]\n",
    "        x = (z + self.bound) / 2.0 / self.bound\n",
    "\n",
    "        # cdf mapping\n",
    "        x, logdet = self._cdf_inv(x)\n",
    "\n",
    "        # maps [0,1] back to [-bound, bound]\n",
    "        x = x * 2.0 * self.bound - self.bound\n",
    "        return x, logdet\n",
    "\n",
    "    # normalize the piecewise representation of pdf\n",
    "    def _pdf_normalize(self):\n",
    "        # peicewise pdf\n",
    "        p0 = torch.ones((1,self.input_dim), dtype=torch.float32, device=self.mesh.device)\n",
    "        self.pdf = p0\n",
    "        px = torch.exp(self.p)*(self.elmt_size[:-1]+self.elmt_size[1:])/2.0\n",
    "        px = (1 - self.elmt_size[0])/torch.sum(px, 0, keepdim=True)\n",
    "        px = px*torch.exp(self.p)\n",
    "        self.pdf = torch.concat([self.pdf, px], 0)\n",
    "        self.pdf = torch.concat([self.pdf, p0], 0)\n",
    "\n",
    "        # probability in each element\n",
    "        cell = (self.pdf[:-1,:] + self.pdf[1:,:])/2.0*self.elmt_size\n",
    "        # CDF - contribution from previous elements.\n",
    "        r_zeros= torch.zeros((1,self.input_dim), dtype=torch.float32, device=self.mesh.device)\n",
    "        self.F_ref = r_zeros\n",
    "        for i in range(1, self.n_bins):\n",
    "            tp  = torch.sum(cell[:i,:], 0, keepdim=True)\n",
    "            self.F_ref = torch.concat([self.F_ref, tp], 0)\n",
    "\n",
    "    # the cdf is a piecewise quadratic function.\n",
    "    def _cdf(self, x):\n",
    "        x_sign = torch.sign(x-0.5)\n",
    "        m = torch.floor(torch.log(torch.abs(x-0.5)*(self.r-1)/self.x1L + 1.0)/np.log(self.r))\n",
    "        k_ind = torch.where(x_sign >= 0, self.n_bins/2 + m, self.n_bins/2 - m - 1)\n",
    "        k_ind = k_ind.to(dtype=torch.int64)\n",
    "        cover = torch.where(k_ind*(k_ind-self.n_bins+1)<=0, 1.0, 0.0)\n",
    "\n",
    "        # print('k_ind', k_ind) \n",
    "        k_ind = torch.where(k_ind < 0, 0*k_ind, k_ind)\n",
    "        k_ind = torch.where(k_ind > (self.n_bins-1), (self.n_bins-1)*torch.ones_like(k_ind), k_ind)\n",
    "\n",
    "        # print(self.pdf[:,0].shape)\n",
    "        \n",
    "        # print(k_ind[:,0])\n",
    "        v1 = torch.reshape(torch.gather(self.pdf[:,0], 0,k_ind[:,0]),(-1,1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(torch.gather(self.pdf[:,i], 0,k_ind[:,i]),(-1,1))\n",
    "            v1 = torch.concat([v1, tp], 1)\n",
    "\n",
    "        v2 = torch.reshape(torch.gather(self.pdf[:,0], 0,k_ind[:,0]+1),(-1,1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(torch.gather(self.pdf[:,i], 0,k_ind[:,i]+1),(-1,1))\n",
    "            v2 = torch.concat([v2, tp], 1)\n",
    "\n",
    "        xmodi = torch.reshape(x[:,0] - torch.gather(self.mesh[:,0], 0,k_ind[:, 0]), (-1, 1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(x[:,i] - torch.gather(self.mesh[:,0], 0,k_ind[:, i]), (-1, 1))\n",
    "            xmodi = torch.concat([xmodi, tp], 1)\n",
    "\n",
    "        h_list = torch.reshape(torch.gather(self.elmt_size[:,0], 0,k_ind[:,0]),(-1,1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(torch.gather(self.elmt_size[:,0], 0,k_ind[:,i]),(-1,1))\n",
    "            h_list = torch.concat([h_list, tp], 1)\n",
    "\n",
    "        F_pre = torch.reshape(torch.gather(self.F_ref[:, 0], 0,k_ind[:, 0]), (-1, 1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(torch.gather(self.F_ref[:, i], 0,k_ind[:, i]), (-1, 1))\n",
    "            F_pre = torch.concat([F_pre, tp], 1)\n",
    "\n",
    "        y = torch.where(cover>0, F_pre + xmodi**2/2.0*(v2-v1)/h_list + xmodi*v1, x)\n",
    "\n",
    "        dlogdet = torch.where(cover > 0, xmodi * (v2 - v1) / h_list + v1, torch.ones_like(cover))\n",
    "        dlogdet = torch.sum(torch.log(dlogdet), dim=[1])\n",
    "\n",
    "        return y, dlogdet\n",
    "\n",
    "    # inverse of the cdf\n",
    "    def _cdf_inv(self, y):\n",
    "        xr = torch.broadcast_to(self.mesh, [self.n_bins+1, self.input_dim])\n",
    "        yr1,_ = self._cdf(xr)\n",
    "\n",
    "        p0 = torch.zeros((1,self.input_dim), device=self.mesh.device,dtype=torch.float32)\n",
    "        p1 = torch.ones((1,self.input_dim), device=self.mesh.device,dtype=torch.float32)\n",
    "        yr = torch.concat([p0, yr1[1:-1,:], p1], 0)\n",
    "\n",
    "        k_ind = torch.searchsorted((yr.T).contiguous(), (y.T).contiguous(), right=True)\n",
    "        k_ind = torch.transpose(k_ind,0,1)\n",
    "        k_ind = k_ind.to(dtype=torch.int64)\n",
    "        k_ind -= 1\n",
    "\n",
    "        cover = torch.where(k_ind*(k_ind-self.n_bins+1) <= 0, 1.0, 0.0)\n",
    "\n",
    "        k_ind = torch.where(k_ind < 0, 0, k_ind)\n",
    "        k_ind = torch.where(k_ind > (self.n_bins-1), self.n_bins-1, k_ind)\n",
    "\n",
    "        c_cover = torch.reshape(cover[:,0], (-1,1))\n",
    "\n",
    "        v1 = torch.where(c_cover > 0, torch.reshape(torch.gather(self.pdf[:,0],0, k_ind[:,0]),(-1,1)), -1.*torch.ones_like(c_cover))\n",
    "        for i in range(1, self.input_dim):\n",
    "            c_cover = torch.reshape(cover[:,i], (-1,1))\n",
    "            tp = torch.where(c_cover > 0, torch.reshape(torch.gather(self.pdf[:,i],0, k_ind[:,i]),(-1,1)), -1.0*torch.ones_like(c_cover))\n",
    "            v1 = torch.concat([v1, tp], 1)\n",
    "\n",
    "        c_cover = torch.reshape(cover[:,0], (-1,1))\n",
    "        v2 = torch.where(c_cover > 0, torch.reshape(torch.gather(self.pdf[:,0],0, k_ind[:,0]+1),(-1,1)), -2.0*torch.ones_like(c_cover))\n",
    "        for i in range(1, self.input_dim):\n",
    "            c_cover = torch.reshape(cover[:,i], (-1,1))\n",
    "            tp = torch.where(c_cover > 0, torch.reshape(torch.gather(self.pdf[:,i],0, k_ind[:,i]+1),(-1,1)), -2.0*torch.ones_like(c_cover))\n",
    "            v2 = torch.concat([v2, tp], 1)\n",
    "\n",
    "        ys = torch.reshape(y[:, 0] - torch.gather(yr[:, 0],0, k_ind[:, 0]), (-1, 1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(y[:, i] - torch.gather(yr[:, i],0, k_ind[:, i]), (-1, 1))\n",
    "            ys = torch.concat([ys, tp], 1)\n",
    "\n",
    "        xs = torch.reshape(torch.gather(xr[:, 0],0, k_ind[:, 0]), (-1, 1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(torch.gather(xr[:, i],0, k_ind[:, i]), (-1, 1))\n",
    "            xs = torch.concat([xs, tp], 1)\n",
    "\n",
    "        h_list = torch.reshape(torch.gather(self.elmt_size[:,0],0, k_ind[:,0]),(-1,1))\n",
    "        for i in range(1, self.input_dim):\n",
    "            tp = torch.reshape(torch.gather(self.elmt_size[:,0],0, k_ind[:,i]),(-1,1))\n",
    "            h_list = torch.concat([h_list, tp], 1)\n",
    "\n",
    "        tp = 2.0*ys*h_list*(v2-v1)\n",
    "        tp += v1*v1*h_list*h_list\n",
    "        tp = torch.sqrt(tp) - v1*h_list\n",
    "        tp = torch.where(torch.abs(v1-v2)<1.0e-6, ys/v1, tp/(v2-v1))\n",
    "        tp += xs\n",
    "\n",
    "        x = torch.where(cover > 0, tp, y)\n",
    "\n",
    "        tp = 2.0 * ys * h_list * (v2 - v1)\n",
    "        tp += v1 * v1 * h_list * h_list\n",
    "        tp = h_list/torch.sqrt(tp)\n",
    "\n",
    "        dlogdet = torch.where(cover > 0, tp, torch.ones_like(cover))\n",
    "        dlogdet = torch.sum(torch.log(dlogdet), dim=[1])\n",
    "\n",
    "        return x, dlogdet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T11:23:17.797244600Z",
     "start_time": "2024-07-15T11:23:17.792117100Z"
    }
   },
   "id": "c4baec875adb58a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义 CDF_quadratic 层\n",
    "n_bins = 10\n",
    "input_dim = 3\n",
    "cdf_layer = CDF_quadratic(n_bins, input_dim)\n",
    "\n",
    "# 示例输入数据\n",
    "x = torch.randn(5, input_dim)\n",
    "\n",
    "# 正向传播\n",
    "y, log_det = cdf_layer(x)\n",
    "print(\"Transformed Output:\", y)\n",
    "print(\"Log-Abs-Det-Jacobian:\", log_det)\n",
    "\n",
    "# 逆向传播\n",
    "x_recovered, inv_log_det = cdf_layer.inverse(y)\n",
    "print(\"Recovered Output:\", x_recovered)\n",
    "print(\"Inverse Log-Abs-Det-Jacobian:\", inv_log_det)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f1c8ba108c003f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
